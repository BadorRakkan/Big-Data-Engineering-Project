{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bd99894-693f-4feb-ac0f-261f4b481731",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## STEP 0. Loading the data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237da7c0-8b08-4c57-90cb-1de663e5ccc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession with the application name \"Table Loading\"\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Table Loading\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dae6439-df52-43f2-8e2c-7004bc0ed9e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 0.1 Creating the `Posts` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ab29ea-342d-4147-a1aa-05a34725ecb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location -- recall our mount storage workshop, data was mounted into '/mnt/deBDProject'\n",
    "file_location = \"/mnt/deBDProject/ml_training/Posts/*\"\n",
    "\n",
    "# Read the Parquet files from the specified file location into a DataFrame\n",
    "posts = spark.read \\\n",
    "  .parquet(file_location)\n",
    "\n",
    "display(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d80f5f4-91be-46d5-a072-8270e38343c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 0.2 Creating the `posttypes` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f1c15e-f803-46d2-9ade-09c8f7e68f31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating the schema for posttypes table\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Creating the schema for the posttypes table\n",
    "PT_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True), # Define the \"id\" field with IntegerType, allowing null values\n",
    "    StructField(\"Type\", StringType(), True) # Define the \"Type\" field with StringType, allowing null values\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1488cedb-c8cf-4dd1-a5e6-505ffdc01551",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating the posttypes dataframe\n",
    "file_location = \"/mnt/deBDProject/ml_training/PostTypes.txt\"\n",
    "\n",
    "# Read the CSV file into a DataFrame with specified options and schema\n",
    "postType = (spark.read\n",
    "  .option(\"header\", \"true\") \n",
    "  .option(\"sep\", \",\") \n",
    "  .schema(PT_schema) \n",
    "  .csv(file_location)) \n",
    "\n",
    "display(postType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ccc477-2d24-4237-9cbc-9842f3a8c4d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 0.3 Creating the `users` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4ee3c0-eaf6-49b6-8c33-303ac4674f88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating the schema for the users table\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema for the users table\n",
    "# The 'True' parameter indicates that the corresponding field is nullable.\n",
    "users_schema = StructType([ \n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"CreationDate\", DateType(), True),\n",
    "    StructField(\"DisplayName\", StringType(), True),\n",
    "    StructField(\"DownVotes\", IntegerType(), True),\n",
    "    StructField(\"EmailHash\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Reputation\", IntegerType(), True),\n",
    "    StructField(\"UpVotes\", IntegerType(), True),\n",
    "    StructField(\"Views\", IntegerType(), True),\n",
    "    StructField(\"WebsiteUrl\", StringType(), True),\n",
    "    StructField(\"AccountId\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c846a81-0190-4c05-b430-d23d4cacd817",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating the users dataframe\n",
    "file_location = \"/mnt/deBDProject/ml_training/users.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame with specified options and schema\n",
    "users = (spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \",\")\n",
    "  .schema(users_schema)\n",
    "  .csv(file_location))\n",
    "\n",
    "display(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9c2fb82-deb7-42fe-913e-c8bf70950440",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 0.4. Saving the dataframes for easy retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "301e8cef-8c14-490c-b8a6-237776ec7808",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the 3 tables to databricks local file system\n",
    "posts.write.parquet(\"/tmp/project/posts.parquet\")\n",
    "postType.write.parquet(\"/tmp/project/PostType.parquet\")\n",
    "users.write.parquet(\"/tmp/project/user.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2966db2-11ef-4bed-a91e-da3bc3a1cde6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# review the local file system\n",
    "display(dbutils.fs.ls(\"/tmp/project/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a2e3dc6-c110-4a7f-a62e-0d81fba9aa03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## STEP 1. Join tables and filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c39ea9ba-555f-46b5-acd5-b7364d36d402",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Prepare necessary libraries and load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076d5740-c157-4562-a3c2-dd7e7e83c56f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7625fdf-05a0-41d6-b0b8-ac9b7fce91af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"ML Model\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39471e6f-d36c-4da3-961e-a749facd70bb",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loads data from Parquet files stored in Azure Data Lake Storage into Spark DataFrames.\n",
    "posts = spark.read.parquet(\"/tmp/project/posts.parquet\")\n",
    "postType = spark.read.parquet(\"/tmp/project/PostType.parquet\")\n",
    "Users = spark.read.parquet(\"/tmp/project/user.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd21382c-7db5-4a46-9dbf-6e788f0485f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Join the tables Posts and postTypes by it post type id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11688769-9f0f-4800-b16d-a0bd0755c5e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform an inner join between the 'posts' DataFrame and the 'postType' DataFrame \n",
    "# based on the equality of 'PostTypeId' column from 'posts' and 'id' column from 'postType'\n",
    "df = posts.join(postType, posts.PostTypeId == postType.id)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "207f240d-b60f-4237-a319-2c9994c083c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3754ef0-b71a-4892-8949-9fb6e71b4210",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame 'df' to retain only rows where the value in the 'Type' column is \"Question\"\n",
    "df = df.filter(col(\"Type\") == \"Question\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45a2b6b-4452-4239-8704-d06b9335d3ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transforming HTML code to strings in the 'Body' column by removing HTML tags\n",
    "# Making a list of the tags by splitting the 'Tags' column based on '<' and '>'\n",
    "df = (df.withColumn('Body', regexp_replace(df.Body, r'<.*?>', '')) # Transforming HTML code to strings\n",
    "      .withColumn(\"Tags\", split(trim(translate(col(\"Tags\"), \"<>\", \" \")), \" \")) # Making a list of the tags\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1010ebc3-444b-44a4-8c22-8e6d6eb9c746",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame 'df' to retain only rows where the value in the 'Type' column is \"Question\"\n",
    "df = df.filter(col(\"Type\") == \"Question\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5743d3d4-f3c7-4004-a015-80e1b9f5c290",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.4 Create a checkpoint to save the dataframe to file only contain the `Body` and `Tag` we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b53d10-93d6-4eb4-bd04-ec89cfbb42e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecting the 'Body' column and renaming it to \"text\", retaining the 'Tags' column\n",
    "df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1f9e88-7048-4f6f-b64a-bc4252a13c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Producing the tags as individual tags instead of an array\n",
    "# This is duplicating the posts for each possible tag\n",
    "df = df.select(\"text\", explode(\"Tags\").alias(\"tags\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf03ecc-da4d-448f-b374-b952f387838f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# saving the file as a checkpoint (in case the cluster gets terminated)\n",
    "df.write.parquet(\"/tmp/project.df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa282575-4697-4f64-b4cc-19ee294e14cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving the dataframe to memory for repetitive use\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a6853e-aeaf-4cb6-9fc2-77616ca6f257",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## STEP 2. Based on the above dataframe, prepare data from machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fedc718c-6016-499f-84bb-003828644739",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1. Text Cleaning Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654d046d-cac1-4bc0-9982-ddae300c69ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# text cleaning and preprocessing steps include removing URLs, special characters, multiple spaces, lowercase all text, and trim whitespaces.\n",
    "cleaned = df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f2ff00-80c1-4db8-8561-b5467ccb04dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## STEP 3. Machine Learning Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a7d9f9b-96dd-4d13-b6bc-8d114106d19e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.1 Feature Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d6130cc-f484-4054-8b52-71242da2f761",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1.1 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a571ca82-c043-40d2-852c-bc98b6d10e67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizing the text data in the 'text' column using the Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Create a Tokenizer instance with inputCol=\"text\" and outputCol=\"tokens\"\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "\n",
    "# Transform the DataFrame 'cleaned' with the tokenizer\n",
    "tokenized = tokenizer.transform(cleaned)\n",
    "\n",
    "display(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8a57dac-f281-4708-93b8-c3d1be7e4a24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1.2 Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc6e661-847d-446c-9cab-1e6a349c4d5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.1.2 Stopword Removal\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Create a StopWordsRemover instance to remove stopwords from tokenized text\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# Apply stopword removal to the tokenized DataFrame\n",
    "stopword = stopword_remover.transform(tokenized)\n",
    "\n",
    "display(stopword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "512814f3-b9ac-4ffd-a96c-148f9868fe3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1.3 CountVectorizer (TF - Term Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5792168-52ca-4ecd-bc23-8fb0fffaeb32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.1.3 CountVectorizer (TF - Term Frequency)\n",
    "# TF - Term Frequency: helps count how many times each word appears in text data and puts those counts into a new column.\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer instance with vocabSize and input/output columns specified\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "\n",
    "# Fit the CountVectorizer to the DataFrame 'stopword' to create a model\n",
    "cv_model = cv.fit(stopword)\n",
    "\n",
    "# Transform the DataFrame 'stopword' using the CountVectorizer model to get the term frequency counts\n",
    "text_cv = cv_model.transform(stopword)\n",
    "\n",
    "display(text_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b0c484-9fb2-4585-9f8b-b93c5963d94e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1.4 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "77b8680d-d918-484f-8427-491b9026df2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.1.4 TF-IDF Vectorization\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# IDF Vectorization helps us figure out how important each word is in our text data and puts those importance scores into a new column.\n",
    "\n",
    "# Create an IDF instance specifying input/output columns and minDocFreq to remove sparse terms\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "# Fit IDF to the DataFrame 'text_cv' to create an IDF model\n",
    "idf_model = idf.fit(text_cv)\n",
    "\n",
    "# Transform the DataFrame 'text_cv' using the IDF model to get the TF-IDF scores\n",
    "text_idf = idf_model.transform(text_cv)\n",
    "\n",
    "display(text_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c963d8-f00a-4dd8-b478-c83dcc4626fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.2 Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7870702d-87d8-4811-9711-3d1a33dc52b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.2 Label Encoding\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create a StringIndexer instance to encode the \"tags\" column into numerical labels\n",
    "label_encoder = StringIndexer(inputCol=\"tags\", outputCol=\"label\")\n",
    "\n",
    "# Fit the StringIndexer to the DataFrame 'text_idf' to create a label encoding model\n",
    "le_model = label_encoder.fit(text_idf)\n",
    "\n",
    "# Transform the DataFrame 'text_idf' using the label encoding model to get the encoded labels\n",
    "final = le_model.transform(text_idf)\n",
    "\n",
    "display(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a05a750-3507-4396-b3fe-05e1a3b99cac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fab327d-70b8-4c3e-afae-4dd2bb875c1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression instance with maxIter specified\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "# Fit the Logistic Regression model to the DataFrame 'final'\n",
    "lr_model = lr.fit(final)\n",
    "\n",
    "# Make predictions using the trained Logistic Regression model on the DataFrame 'final'\n",
    "predictions = lr_model.transform(final)\n",
    "\n",
    "display(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9ff2ab7-b0c6-4c86-a8cb-9b09b7e3fdd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.4 Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182fe1d1-df86-4412-8f37-b6200f2a2578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a MulticlassClassificationEvaluator instance specifying the column for predictions\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate the ROC AUC score using the evaluator on the 'predictions' DataFrame\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Calculate the accuracy by counting correct predictions and dividing by the total number of predictions\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bc97ff4-e313-4832-a385-6ff26bb52f56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.5 Create a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668babc0-599d-450f-82bf-8336c30411ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing all the libraries\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Preparing the data\n",
    "# Step 1: Creating the joined table\n",
    "df = posts.join(postType, posts.PostTypeId == postType.id)\n",
    "# Step 2: Selecting only Question posts\n",
    "df = df.filter(col(\"Type\") == \"Question\")\n",
    "# Step 3: Formatting the raw data\n",
    "df = (df.withColumn('Body', regexp_replace(df.Body, r'<.*?>', ''))\n",
    "      .withColumn(\"Tags\", split(trim(translate(col(\"Tags\"), \"<>\", \" \")), \" \"))\n",
    ")\n",
    "# Step 4: Selecting the columns\n",
    "df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"))\n",
    "# Step 5: Getting the tags\n",
    "df = df.select(\"text\", explode(\"Tags\").alias(\"tags\"))\n",
    "# Step 6: Clean the text\n",
    "cleaned = df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n",
    "\n",
    "# Machine Learning\n",
    "# Step 1: Train Test Split\n",
    "train, test = cleaned.randomSplit([0.9, 0.1], seed=20200819)\n",
    "# Step 2: Initializing the transfomers\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "label_encoder = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "# Step 3: Creating the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, label_encoder, lr])\n",
    "# Step 4: Fitting and transforming (predicting) using the pipeline\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6257b7-e8bf-450d-b743-5b366b64701b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## STEP 4. Save the Model file to Azure storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70deea7-bad9-417a-ba35-a40dbfa5616a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving model object to the /mnt/deBDProject directory. Yours name may be different.\n",
    "pipeline_model.save('/mnt/deBDProject/model')\n",
    "\n",
    "# Save the the String Indexer to decode the encoding. We need it in the future Sentiment Analysis.\n",
    "le_model.save('/mnt/deBDProject/stringindexer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a7cb7c-eab3-4266-af37-74c25c2be3f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Review the directory\n",
    "display(dbutils.fs.ls(\"/mnt/deBDProject/model\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Model Training",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
