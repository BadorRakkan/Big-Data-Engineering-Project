{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fb7e346-164a-40bd-b455-76ce805a0401",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d123abca-2c7b-4e5d-a740-f458a0c8fe9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Spark preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dd52a9-9feb-457b-9aca-0f6f88721e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libaries\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473c7dff-6e6f-4f3c-bf48-4283559e32fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"ML Model\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a3afe79-00d5-4dac-be4a-c40b5da38381",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Prepare a UDF (User Defined Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a28084d-6eaf-4115-90b5-4f19a80bf8a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# User defined function\n",
    "def predictions_udf(df, ml_model, stringindexer):\n",
    "    from pyspark.sql.functions import col, regexp_replace, lower, trim\n",
    "    from pyspark.ml import PipelineModel\n",
    "\n",
    "    # Filter out empty body text\n",
    "    df = df.filter(\"Body is not null\")\n",
    "    # Making sure the naming of the columns are consistent with the model\n",
    "    df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"))\n",
    "    # Preprocessing of the feature column\n",
    "    cleaned = df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n",
    "\n",
    "    # Load in the saved pipeline model\n",
    "    model = PipelineModel.load(ml_model)\n",
    "\n",
    "    # Making the prediction\n",
    "    prediction = model.transform(df)\n",
    "\n",
    "    predicted = prediction.select(col('text'), col('Tags'), col('prediction'))\n",
    "\n",
    "    # Decoding the indexer\n",
    "    from pyspark.ml.feature import StringIndexerModel, IndexToString\n",
    "\n",
    "    # Load in the StringIndexer that was saved\n",
    "    indexer = StringIndexerModel.load(stringindexer)\n",
    "\n",
    "    # Initialize the IndexToString converter\n",
    "    i2s = IndexToString(inputCol = 'prediction', outputCol = 'decoded', labels = indexer.labels)\n",
    "    converted = i2s.transform(predicted)\n",
    "\n",
    "    # Display the important columns\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c86d96d-78da-4a1d-b0ad-5c612866db41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Load Posts files and ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf704e3-f86f-4d04-acba-bbf017f36852",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/deBDProject/model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f1e206-3180-4d97-a996-e2536d938626",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "posts = spark.read.parquet(\"/mnt/deBDProject/landing/Posts/*\")\n",
    "ml_model = \"/mnt/deBDProject/model\"\n",
    "stringindexer = \"/mnt/deBDProject/stringindexer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "851b5af0-89cb-4549-9388-c2450e39b050",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.4 Run model to do `Sentiment Analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea13897-0bb7-4896-af18-e8fca83b9de6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = predictions_udf(posts,ml_model, stringindexer)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf6a0fe-a12a-42a0-bcb1-aaa7c002ee5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.5 Summarize which topics are the most popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a877c67d-3f25-4794-a270-c97573fc9d13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# change the column name \n",
    "topics = result.withColumnRenamed('decoded', 'topic').select('topic')\n",
    "\n",
    "# Aggregate the topics and calculate the total qty of each topic\n",
    "topic_qty = topics.groupBy(col(\"topic\")).agg(count('topic').alias('qty')).orderBy(desc('qty'))\n",
    "topic_qty.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5c19ff2-65bf-45d8-b28f-7f41615fc2a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.6 Save the result file to the `BI` folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b5debd4-7fef-4950-a8dd-af1d5ce88974",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define this function\n",
    "\n",
    "def crt_sgl_file(result_path):\n",
    "        # write the result to a folder container several files\n",
    "        path = \"/mnt/deBDProject/BI/ml_result\"\n",
    "        topic_qty.write.option(\"delimiter\", \",\").option(\"header\", \"true\").mode(\"overwrite\").csv(path)\n",
    "\n",
    "        # list the folder, find the csv file \n",
    "        filenames = dbutils.fs.ls(path)\n",
    "        name = ''\n",
    "        for filename in filenames:\n",
    "            if filename.name.endswith('csv'):\n",
    "                org_name = filename.name\n",
    "\n",
    "        # copy the csv file to the path you want to save, in this example, we use  \"/mnt/deBDProject/BI/ml_result.csv\"\n",
    "        dbutils.fs.cp(path + '/'+ org_name, result_path)\n",
    "\n",
    "        # delete the folder\n",
    "        dbutils.fs.rm(path, True)\n",
    "\n",
    "        print('single file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc1d99c-a3ad-425d-bf97-a3d75d52f552",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run the function\n",
    "result_path = \"/mnt/deBDProject/BI/ml_result.csv\"\n",
    "\n",
    "crt_sgl_file(result_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML Sentiment Analysis",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
